# ============================================================================
# DeBERTa-v3 Model Architecture Configuration
# ============================================================================
# DeBERTa-v3-base: State-of-the-art NLI performance with disentangled attention
# Superior to BERT/RoBERTa on MNLI, SNLI, and other NLI benchmarks
# ============================================================================

# model_name: HuggingFace model identifier
#   - microsoft/deberta-v3-base (12-layer, 768-hidden, 6M vocab, 184M params)
#   - DeBERTa (Decoding-enhanced BERT with disentangled attention)
#   - v3: Enhanced mask decoder + replaced token detection pretraining
#   - Superior NLI performance due to disentangled attention mechanism
#   - Does NOT use token_type_ids (automatically detected)
model_name: microsoft/deberta-v3-base

# num_labels: Number of output classes for classification head
#   - Default: 2 (binary classification: match vs no-match)
num_labels: 2

# dropout: Dropout probability for classification head
#   - Default: 0.1 (10% dropout rate)
#   - DeBERTa-v3 is robust to overfitting, can use standard 0.1
dropout: 0.1

# freeze_bert: Whether to freeze encoder weights during training
#   - false: Fine-tune all layers (recommended for best performance)
#   - DeBERTa-v3 benefits significantly from fine-tuning
freeze_bert: false
