# ============================================================================
# RoBERTa Model Architecture Configuration
# ============================================================================
# RoBERTa-base: Robustly Optimized BERT Pretraining Approach
# Improved BERT with better pretraining (no NSP, dynamic masking, larger batches)
# ============================================================================

# model_name: HuggingFace model identifier
#   - FacebookAI/roberta-base (12-layer, 768-hidden, 50K vocab, 125M params)
#   - RoBERTa improvements over BERT:
#     * Trained longer with larger batches
#     * Dynamic masking pattern (different each epoch)
#     * Removed Next Sentence Prediction (NSP) task
#     * Uses byte-level BPE tokenization
#   - Does NOT use token_type_ids (automatically detected)
#   - Uses <s> and </s> instead of [CLS] and [SEP]
model_name: FacebookAI/roberta-base

# num_labels: Number of output classes for classification head
#   - Default: 2 (binary classification: match vs no-match)
num_labels: 2

# dropout: Dropout probability for classification head
#   - Default: 0.1 (10% dropout rate)
#   - RoBERTa uses same dropout as BERT
dropout: 0.1

# freeze_bert: Whether to freeze encoder weights during training
#   - false: Fine-tune all layers (recommended)
#   - RoBERTa's robust pretraining benefits from fine-tuning
freeze_bert: false
