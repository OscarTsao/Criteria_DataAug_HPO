# ============================================================================
# ModernBERT Model Architecture Configuration
# ============================================================================
# ModernBERT-base: Latest generation encoder (December 2024)
# Modernized architecture with 8K context, Flash Attention, RoPE embeddings
# ============================================================================

# model_name: HuggingFace model identifier
#   - answerdotai/ModernBERT-base (22-layer, 768-hidden, 149M params)
#   - Released December 2024 by Answer.AI
#   - Key improvements:
#     * 8K context length (vs 512 for BERT)
#     * Flash Attention 2 for efficiency
#     * Rotary Position Embeddings (RoPE) instead of learned
#     * Alternating global/local attention
#     * GeGLU activation instead of GELU
#   - Does NOT use token_type_ids (automatically detected)
#   - Requires transformers>=4.48.0
model_name: answerdotai/ModernBERT-base

# num_labels: Number of output classes for classification head
#   - Default: 2 (binary classification: match vs no-match)
num_labels: 2

# dropout: Dropout probability for classification head
#   - Default: 0.1 (10% dropout rate)
#   - ModernBERT uses standard dropout
dropout: 0.1

# freeze_bert: Whether to freeze encoder weights during training
#   - false: Fine-tune all layers (recommended)
#   - ModernBERT benefits from fine-tuning like other encoders
freeze_bert: false

# requires_transformers: Minimum transformers version required
#   - ModernBERT needs transformers>=4.48.0 for architecture support
requires_transformers: "4.48.0"
