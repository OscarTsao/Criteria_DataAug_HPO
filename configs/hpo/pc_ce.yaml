# ============================================================================
# P–C Cross-Encoder HPO (BGE Reranker) with LoRA/QLoRA knobs and post-hoc eval
# ============================================================================
defaults:
  - _self_

# Core Optuna settings used by the existing CLI harness
n_trials: 500
storage: ${oc.env:OPTUNA_STORAGE,sqlite:///optuna.db}
study_name: pc_ce_hpo
direction: maximize

pruner:
  type: MedianPruner
  n_startup_trials: 5
  n_warmup_steps: 1
  interval_steps: 1

# ---------------------------------------------------------------------------
# Search space (Optuna-style); condition keys are informational for now
# ---------------------------------------------------------------------------
search_space:
  # --- Core training knobs ---
  learning_rate:
    type: loguniform
    low: 7.5e-6
    high: 2.0e-5
  weight_decay:
    type: uniform
    low: 0.0
    high: 0.10
  warmup_ratio:
    type: uniform
    low: 0.02
    high: 0.10
  scheduler_type:
    type: categorical
    choices: [linear, cosine, cosine_with_restarts]
  target_effective_batch_size:
    type: categorical
    choices: [32, 64, 128, 256, 512]
  amp_dtype:
    type: categorical
    choices: [bf16]
  grad_clip:
    type: fixed
    value: 1.0

  # --- LoRA / QLoRA ---
  lora_r:
    type: categorical
    choices: [16, 32]
  lora_alpha:
    type: categorical
    choices: [16, 32, 64]
  lora_dropout:
    type: uniform
    low: 0.00
    high: 0.15
  lora_targets:
    type: categorical
    choices: [qkv, qkvd]
  qlora_bits:
    type: categorical
    choices: [4bit, 8bit, none]

  # --- Evidence-only augmentation ---
  aug_enable:
    type: categorical
    choices: [true, false]
  aug_prob:
    type: uniform
    low: 0.10
    high: 0.50
    condition: "aug_enable==true"
  aug_method:
    type: categorical
    choices: [synonym, contextual]
    condition: "aug_enable==true"

  # --- Input shaping / segmentation ---
  chunk_max_len:
    type: categorical
    choices: [512, 384]
  chunk_stride:
    type: categorical
    choices: [128, 192]
  pc_topM_enable:
    type: categorical
    choices: [false, true]
  pc_topM_M:
    type: categorical
    choices: [6, 8, 10]
    condition: "pc_topM_enable==true"

  # --- Loss & sampling (class imbalance) ---
  loss_type:
    type: categorical
    choices: [bce+rank, cb_focal]

  pos_weight_scale:
    type: uniform
    low: 1.0
    high: 6.0
    condition: "loss_type==bce+rank"
  rank_margin:
    type: uniform
    low: 0.10
    high: 0.40
    condition: "loss_type==bce+rank"
  rank_weight:
    type: uniform
    low: 0.20
    high: 0.70
    condition: "loss_type==bce+rank"
  label_smoothing:
    type: uniform
    low: 0.00
    high: 0.10
    condition: "loss_type==bce+rank"

  focal_gamma:
    type: categorical
    choices: [1, 2, 3]
    condition: "loss_type==cb_focal"
  cb_beta:
    type: uniform
    low: 0.995
    high: 0.9995
    condition: "loss_type==cb_focal"

  neg_per_pos:
    type: categorical
    choices: [2, 4, 6, 8]
  xpost_neg_frac:
    type: uniform
    low: 0.0
    high: 0.3

  # --- Global aggregation for P–C ---
  agg_type:
    type: categorical
    choices: [LSE, "top-m", max]
  lse_tau:
    type: uniform
    low: 5.0
    high: 10.0
    condition: "agg_type==LSE"
  top_m:
    type: categorical
    choices: [2, 3]
    condition: "agg_type=='top-m'"

  # --- Threshold tuning (post-hoc) ---
  threshold_mode:
    type: categorical
    choices: [per_class, global]
  threshold_global_init:
    type: uniform
    low: 0.30
    high: 0.70
    condition: "threshold_mode==global"

# ---------------------------------------------------------------------------
# Fixed settings and evaluation contract (informational for pipeline wiring)
# ---------------------------------------------------------------------------
fixed:
  model_name: "BAAI/bge-reranker-v2-m3"
  tokenizer_truncation: "only_first"
  optimizer: "adamw"
  seed: 42
  early_stopping:
    metric: "macro_f1_post_calibrated"
    mode: "max"
    patience: 20
    min_delta: 0.001
  calibration:
    enable: true
    method: "temperature"
  thresholds:
    mode: "per_class"  # preferred default; can be overridden by search_space.threshold_mode
  objective:
    primary: "macro_f1_post_calibrated"
    secondary: ["macro_auprc_post", "ece_post", "p95_latency_ms"]
    latency_budget_ms:
      enabled: true
      p95_limit_relative: 0.30  # +30% vs baseline if pc_topM_enable is true
  hpo:
    n_trials: 500
    pruner: "MedianPruner"
    prune_after_n_steps: 1
    multi_seed_topk: 5

  defaults_if_no_hpo:
    learning_rate: 1.5e-5
    weight_decay: 0.01
    warmup_ratio: 0.06
    epochs: 100
    batch_size: 16
    grad_accum: 2
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    lora_targets: qkvd
    qlora_bits: 4bit
    loss_type: bce+rank
    pos_weight_scale: 3.0
    rank_weight: 0.5
    rank_margin: 0.2
    label_smoothing: 0.0
    chunk_max_len: 512
    chunk_stride: 128
    pc_topM_enable: false
    agg_type: LSE
    lse_tau: 8
