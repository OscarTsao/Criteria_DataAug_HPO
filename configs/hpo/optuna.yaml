# ============================================================================
# Optuna Hyperparameter Optimization Configuration
# ============================================================================
# Defines HPO study settings, pruning strategy, and search space
# Run via: python -m Project.cli command=hpo n_trials=500
# ============================================================================

# ============================================================================
# OPTUNA STUDY SETTINGS
# ============================================================================

# n_trials: Number of hyperparameter combinations to evaluate
#   - Default: 500 (paired with 100-epoch folds and patience 20)
#   - Each trial runs full 100-epoch K-fold CV (patience 20)
#   - Higher = better hyperparameters but longer runtime
#   - Typical values: 100-500 for medium search spaces; >500 for exhaustive search
n_trials: 2000

# storage: Database backend for storing trial history
#   - PostgreSQL recommended for parallel HPO (better concurrency)
#   - Format: postgresql://user:password@host:port/database
#   - Override via CLI: hpo.storage=postgresql://user:pass@host:5432/optuna
#   - Or use environment variables: ${oc.env:POSTGRES_URL}
#   - Alternatives:
#     * sqlite:///optuna.db (local SQLite file, limited concurrency) [default]
#     * mysql://user:pass@host/db (production alternative)
#   - Allows resuming interrupted HPO studies
#   - Enables parallel optimization across multiple processes
#   - Default: SQLite for ease of use; set OPTUNA_STORAGE to Postgres when available
storage: ${oc.env:OPTUNA_STORAGE,sqlite:///optuna.db}

# study_name: Unique identifier for this optimization study
#   - Used to group related trials together
#   - Can resume study with same name: load_if_exists=True
#   - Best practice: descriptive name like "dsm5_bge_reranker_v1"
study_name: project_hpo

# direction: Optimization direction for objective metric
#   - maximize: For metrics like F1, accuracy, AUC (default)
#   - minimize: For loss, error rate
#   - This study maximizes mean F1 across K-fold validation
direction: maximize

# ============================================================================
# HPO MODE CONFIGURATION
# ============================================================================
# Controls how HPO evaluates each trial (single-split vs K-fold)
hpo_mode:
  # mode: Validation strategy during HPO
  #   - single_split: Use single train/val split (MUCH FASTER, ~10-15x speedup)
  #     * Best for: Initial hyperparameter search
  #     * Speed: Each trial completes in minutes instead of hours
  #     * After HPO: Run final K-fold training with best hyperparameters
  #   - kfold: Use full K-fold cross-validation (SLOWER, more reliable)
  #     * Best for: Final validation, small search spaces
  #     * Speed: Each trial takes hours (5 folds Ã— many epochs)
  mode: single_split  # single_split (recommended for 2000 trials) or kfold

  # train_split: Train/val split ratio for single_split mode
  #   - Default: 0.8 (80% train, 20% val)
  #   - Post-level grouping maintained to prevent leakage
  #   - Stratified by label distribution
  train_split: 0.8

  # num_epochs: Number of epochs for HPO trials (overrides training.num_epochs during HPO)
  #   - single_split: 30-50 epochs sufficient (focus on finding good params quickly)
  #   - kfold: 100 epochs (full training for reliable comparison)
  #   - Lower epochs during HPO = faster iteration, more trials explored
  num_epochs: 40

  # early_stopping_patience: Early stopping patience during HPO
  #   - single_split: 10 epochs (stop bad trials quickly)
  #   - kfold: 20 epochs (more conservative for reliable results)
  #   - Lower patience = faster pruning of bad hyperparameters
  early_stopping_patience: 10

# ============================================================================
# PRUNING CONFIGURATION
# ============================================================================
# Early stopping for unpromising trials to save computation time
pruner:
  # type: Pruning algorithm
  #   - HyperbandPruner: Successive halving with aggressive early stopping
  #   - More efficient than MedianPruner for large trial counts (2000+)
  type: HyperbandPruner

  # min_resource: Minimum resource before first pruning check
  #   - For single_split mode: Minimum epochs (e.g., 5 epochs)
  #   - For kfold mode: Minimum folds (e.g., 1 fold)
  #   - Aggressive early stopping for unpromising trials
  min_resource: 5

  # max_resource: Maximum resource allocation
  #   - For single_split mode: Max epochs (40 epochs as per hpo_mode.num_epochs)
  #   - For kfold mode: Max folds (5 folds as per kfold.n_splits)
  #   - Top trials get full resource allocation
  max_resource: 40

  # reduction_factor: Successive halving reduction factor (eta)
  #   - Default: 3 (keep top 1/3 of trials at each stage)
  #   - Stages: 5 epochs -> 10 epochs -> 20 epochs -> 40 epochs
  #   - Balanced pruning for epoch-level optimization
  reduction_factor: 3

  # bootstrap_count: Number of trials before pruning starts
  #   - Default: 30 (gather sufficient baseline for 2000 trials)
  #   - Higher for large-scale HPO to establish reliable pruning baseline
  #   - ~1.5% of total trials used for bootstrap
  bootstrap_count: 30

# ============================================================================
# HYPERPARAMETER SEARCH SPACE
# ============================================================================
# Defines ranges for each hyperparameter to optimize
# Note: type field is for documentation only (actual sampling in cli.py)

search_space:
  # learning_rate: AdamW learning rate
  #   - type: loguniform - samples from log scale (better for orders of magnitude)
  #   - Range: [1e-6, 5e-5] - expanded range for comprehensive search
  #   - Examples: 1e-6, 5e-6, 1e-5, 2e-5, 5e-5
  #   - Log scale ensures good coverage across orders of magnitude
  learning_rate:
    type: loguniform
    low: 1e-6
    high: 5e-5

  # target_effective_batch_size: Effective batch size after gradient accumulation
  #   - type: categorical - discrete choices only
  #   - Choices: [16, 32, 64, 128, 256] - controls optimization dynamics
  #   - Physical batch size is auto-detected per GPU
  #   - Gradient accumulation steps calculated automatically
  #   - 16: Very conservative, fastest iterations
  #   - 32: Conservative baseline
  #   - 64: Balanced (default)
  #   - 128: Large batches, stable gradients
  #   - 256: Very large batches, maximum stability
  target_effective_batch_size:
    type: categorical
    choices: [16, 32, 64, 128, 256]

  # scheduler_type: Learning rate scheduler
  #   - type: categorical - discrete choices
  #   - Choices: [linear, cosine, cosine_with_restarts]
  #   - linear: Constant decay (default, stable)
  #   - cosine: Smooth cosine decay (popular for transformers)
  #   - cosine_with_restarts: Multiple cycles (helps escape local minima)
  scheduler_type:
    type: categorical
    choices: [linear, cosine, cosine_with_restarts]

  # weight_decay: L2 regularization strength for AdamW
  #   - type: uniform - linear scale to include 0 (no regularization)
  #   - Range: [0.0, 0.1] - includes no regularization to full regularization
  #   - Examples: 0.0, 0.001, 0.01, 0.05, 0.1
  #   - 0 = no regularization, higher values prevent overfitting
  weight_decay:
    type: uniform
    low: 0.0
    high: 0.1

  # warmup_ratio: Fraction of steps for LR warmup
  #   - type: uniform - linear sampling from range
  #   - Range: [0.0, 0.2] - 0% to 20% of total steps
  #   - 0 = no warmup (may converge faster but less stable)
  #   - Higher = longer warmup (more stable, slower convergence)
  #   - Warmup prevents initial instability in BERT fine-tuning
  warmup_ratio:
    type: uniform
    low: 0.0
    high: 0.2

  # classifier_dropout: Dropout rate for classifier head
  #   - type: uniform - linear sampling from range
  #   - Range: [0.1, 0.5] - moderate to high dropout for regularization
  #   - Higher = more regularization, prevents overfitting
  #   - Applied to classification layer before final output
  classifier_dropout:
    type: uniform
    low: 0.1
    high: 0.5

  # hidden_dropout: Dropout rate for transformer hidden layers
  #   - type: uniform - linear sampling from range
  #   - Range: [0.0, 0.3] - low to moderate dropout
  #   - Applied throughout transformer encoder layers
  #   - 0 = no dropout, higher values increase regularization
  hidden_dropout:
    type: uniform
    low: 0.0
    high: 0.3

  # attention_dropout: Dropout rate for attention weights
  #   - type: uniform - linear sampling from range
  #   - Range: [0.0, 0.3] - low to moderate dropout
  #   - Applied to attention probabilities in multi-head attention
  #   - Helps prevent attention overfitting
  attention_dropout:
    type: uniform
    low: 0.0
    high: 0.3

  # focal_gamma: Focusing parameter for focal loss
  #   - type: categorical - discrete values
  #   - Choices: [1.0, 2.0, 3.0] - controls focusing strength
  #   - 1.0 = similar to cross-entropy
  #   - 2.0 = standard focal loss (default)
  #   - 3.0 = aggressive focusing on hard examples
  focal_gamma:
    type: categorical
    choices: [1.0, 2.0, 3.0]
