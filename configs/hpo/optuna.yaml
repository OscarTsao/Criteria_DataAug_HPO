# ============================================================================
# Optuna Hyperparameter Optimization Configuration
# ============================================================================
# Defines HPO study settings, pruning strategy, and search space
# Run via: python -m Project.cli command=hpo n_trials=500
# ============================================================================

# ============================================================================
# OPTUNA STUDY SETTINGS
# ============================================================================

# n_trials: Number of hyperparameter combinations to evaluate
#   - Default: 500 (paired with 100-epoch folds and patience 20)
#   - Each trial runs full 100-epoch K-fold CV (patience 20)
#   - Higher = better hyperparameters but longer runtime
#   - Typical values: 100-500 for medium search spaces; >500 for exhaustive search
n_trials: 2000

# storage: Database backend for storing trial history
#   - PostgreSQL recommended for parallel HPO (better concurrency)
#   - Format: postgresql://user:password@host:port/database
#   - Override via CLI: hpo.storage=postgresql://user:pass@host:5432/optuna
#   - Or use environment variables: ${oc.env:POSTGRES_URL}
#   - Alternatives:
#     * sqlite:///optuna.db (local SQLite file, limited concurrency) [default]
#     * mysql://user:pass@host/db (production alternative)
#   - Allows resuming interrupted HPO studies
#   - Enables parallel optimization across multiple processes
#   - Default: SQLite for ease of use; set OPTUNA_STORAGE to Postgres when available
storage: ${oc.env:OPTUNA_STORAGE,sqlite:///optuna.db}

# study_name: Unique identifier for this optimization study
#   - Used to group related trials together
#   - Can resume study with same name: load_if_exists=True
#   - Best practice: descriptive name like "dsm5_bge_reranker_v1"
study_name: project_hpo

# direction: Optimization direction for objective metric
#   - maximize: For metrics like F1, accuracy, AUC (default)
#   - minimize: For loss, error rate
#   - This study maximizes mean F1 across K-fold validation
direction: maximize

# ============================================================================
# PRUNING CONFIGURATION
# ============================================================================
# Early stopping for unpromising trials to save computation time
pruner:
  # type: Pruning algorithm
  #   - HyperbandPruner: Successive halving with aggressive early stopping
  #   - More efficient than MedianPruner for large trial counts (2000+)
  type: HyperbandPruner

  # min_resource: Minimum number of folds before first pruning check
  #   - Default: 1 (can prune after first fold)
  #   - Aggressive early stopping for unpromising trials
  min_resource: 1

  # max_resource: Maximum number of folds (K in K-fold CV)
  #   - Default: 5 (5-fold cross-validation)
  #   - Top trials complete all 5 folds
  max_resource: 5

  # reduction_factor: Successive halving reduction factor (eta)
  #   - Default: 4 (keep top 1/4 of trials at each stage)
  #   - Aggressive pruning for 2000-trial optimization
  #   - Higher = faster elimination, lower = more conservative
  reduction_factor: 4

  # bootstrap_count: Number of trials before pruning starts
  #   - Default: 30 (gather sufficient baseline for 2000 trials)
  #   - Higher for large-scale HPO to establish reliable pruning baseline
  #   - ~1.5% of total trials used for bootstrap
  bootstrap_count: 30

# ============================================================================
# HYPERPARAMETER SEARCH SPACE
# ============================================================================
# Defines ranges for each hyperparameter to optimize
# Note: type field is for documentation only (actual sampling in cli.py)

search_space:
  # learning_rate: AdamW learning rate
  #   - type: loguniform - samples from log scale (better for orders of magnitude)
  #   - Range: [1e-6, 5e-5] - expanded range for comprehensive search
  #   - Examples: 1e-6, 5e-6, 1e-5, 2e-5, 5e-5
  #   - Log scale ensures good coverage across orders of magnitude
  learning_rate:
    type: loguniform
    low: 1e-6
    high: 5e-5

  # batch_size: Training batch size
  #   - type: categorical - discrete choices only
  #   - Choices: [16, 32, 64, 128] - Updated for larger batch search
  #   - Auto VRAM detection handles gradient accumulation if needed
  #   - 16: Conservative baseline
  #   - 32: Moderate (typical)
  #   - 64: Large batch training
  #   - 128: Very large batch (uses gradient accumulation)
  batch_size:
    type: categorical
    choices: [16, 32, 64, 128]

  # weight_decay: L2 regularization strength for AdamW
  #   - type: uniform - linear scale to include 0 (no regularization)
  #   - Range: [0.0, 0.1] - includes no regularization to full regularization
  #   - Examples: 0.0, 0.001, 0.01, 0.05, 0.1
  #   - 0 = no regularization, higher values prevent overfitting
  weight_decay:
    type: uniform
    low: 0.0
    high: 0.1

  # warmup_ratio: Fraction of steps for LR warmup
  #   - type: uniform - linear sampling from range
  #   - Range: [0.0, 0.2] - 0% to 20% of total steps
  #   - 0 = no warmup (may converge faster but less stable)
  #   - Higher = longer warmup (more stable, slower convergence)
  #   - Warmup prevents initial instability in BERT fine-tuning
  warmup_ratio:
    type: uniform
    low: 0.0
    high: 0.2

  # classifier_dropout: Dropout rate for classifier head
  #   - type: uniform - linear sampling from range
  #   - Range: [0.1, 0.5] - moderate to high dropout for regularization
  #   - Higher = more regularization, prevents overfitting
  #   - Applied to classification layer before final output
  classifier_dropout:
    type: uniform
    low: 0.1
    high: 0.5

  # hidden_dropout: Dropout rate for transformer hidden layers
  #   - type: uniform - linear sampling from range
  #   - Range: [0.0, 0.3] - low to moderate dropout
  #   - Applied throughout transformer encoder layers
  #   - 0 = no dropout, higher values increase regularization
  hidden_dropout:
    type: uniform
    low: 0.0
    high: 0.3

  # attention_dropout: Dropout rate for attention weights
  #   - type: uniform - linear sampling from range
  #   - Range: [0.0, 0.3] - low to moderate dropout
  #   - Applied to attention probabilities in multi-head attention
  #   - Helps prevent attention overfitting
  attention_dropout:
    type: uniform
    low: 0.0
    high: 0.3

  # focal_gamma: Focusing parameter for focal loss
  #   - type: categorical - discrete values
  #   - Choices: [1.0, 2.0, 3.0] - controls focusing strength
  #   - 1.0 = similar to cross-entropy
  #   - 2.0 = standard focal loss (default)
  #   - 3.0 = aggressive focusing on hard examples
  focal_gamma:
    type: categorical
    choices: [1.0, 2.0, 3.0]
