diff --git a/pyproject.toml b/pyproject.toml
index 02e3313..218f38b 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -17,7 +17,7 @@ classifiers = [
 ]
 dependencies = [
   "torch>=2.0.0",
-  "transformers>=4.30.0",
+  "transformers>=4.48.0",
   "mlflow>=2.5.0",
   "optuna>=3.4.0",
   "hydra-core>=1.3.0",
diff --git a/src/Project/cli.py b/src/Project/cli.py
index aec129f..a99fec9 100644
--- a/src/Project/cli.py
+++ b/src/Project/cli.py
@@ -65,12 +65,14 @@ def run_single_fold(
         tokenizer,
         max_length=config.data.max_length,
         verify_format=(fold == 0),  # Verify format only for first fold
+        model_name=config.model.model_name,
     )

     val_dataset = DSM5NLIDataset(
         pairs_df.iloc[val_idx],
         tokenizer,
         max_length=config.data.max_length,
+        model_name=config.model.model_name,
     )

     # Create dataloaders
@@ -283,12 +285,14 @@ def run_hpo(config: DictConfig, n_trials: int):
                     pairs_df.iloc[train_idx],
                     tokenizer,
                     max_length=config.data.max_length,
+                    model_name=config.model.model_name,
                 )

                 val_dataset = DSM5NLIDataset(
                     pairs_df.iloc[val_idx],
                     tokenizer,
                     max_length=config.data.max_length,
+                    model_name=config.model.model_name,
                 )

                 # Create dataloaders with trial batch size
diff --git a/src/Project/data/dataset.py b/src/Project/data/dataset.py
index fd6d596..c0559d9 100644
--- a/src/Project/data/dataset.py
+++ b/src/Project/data/dataset.py
@@ -20,6 +20,7 @@ class DSM5NLIDataset(Dataset):
         tokenizer,
         max_length: int = 512,
         verify_format: bool = False,
+        model_name: str = None,
     ):
         """Initialize dataset.

@@ -28,6 +29,7 @@ class DSM5NLIDataset(Dataset):
             tokenizer: HuggingFace tokenizer
             max_length: Maximum sequence length
             verify_format: Whether to validate column presence (debug helper)
+            model_name: Model name for detecting token_type_ids support (optional)
         """
         if verify_format:
             required_columns = {"post", "criterion", "label"}
@@ -41,6 +43,11 @@ class DSM5NLIDataset(Dataset):
         self.tokenizer = tokenizer
         self.max_length = max_length

+        # Detect if tokenizer produces token_type_ids
+        # RoBERTa, DeBERTa, ModernBERT tokenizers don't generate them
+        test_encoding = self.tokenizer("test", "test", return_tensors="pt")
+        self.has_token_type_ids = "token_type_ids" in test_encoding
+
     def __len__(self) -> int:
         return len(self.data)

@@ -58,12 +65,18 @@ class DSM5NLIDataset(Dataset):
             return_tensors="pt",
         )

-        return {
+        item = {
             "input_ids": encoding["input_ids"].squeeze(0),
             "attention_mask": encoding["attention_mask"].squeeze(0),
             "labels": torch.tensor(row["label"], dtype=torch.long),
         }

+        # Only include token_type_ids if the tokenizer produces them
+        if self.has_token_type_ids and "token_type_ids" in encoding:
+            item["token_type_ids"] = encoding["token_type_ids"].squeeze(0)
+
+        return item
+

 def create_dataloaders(train_dataset, val_dataset, batch_size: int,
                       num_workers: int = 4, pin_memory: bool = True):
diff --git a/src/Project/models/bert_classifier.py b/src/Project/models/bert_classifier.py
index 7e781a2..8c61f94 100644
--- a/src/Project/models/bert_classifier.py
+++ b/src/Project/models/bert_classifier.py
@@ -33,6 +33,17 @@ class BERTClassifier(nn.Module):
         self.bert = AutoModel.from_pretrained(model_name)
         self.config = AutoConfig.from_pretrained(model_name)

+        # Detect model capabilities for multi-model support
+        # Some models (RoBERTa, DeBERTa, ModernBERT) don't use token_type_ids
+        self.uses_token_type_ids = (
+            hasattr(self.config, 'type_vocab_size') and
+            self.config.type_vocab_size > 0
+        )
+
+        # Some models don't have pooler_output, need to use CLS token manually
+        # This will be checked dynamically in forward pass
+        self.has_pooler = None  # Determined on first forward pass
+
         # Freeze BERT if requested
         if freeze_bert:
             for param in self.bert.parameters():
@@ -46,12 +57,34 @@ class BERTClassifier(nn.Module):
         nn.init.xavier_uniform_(self.classifier.weight)
         nn.init.zeros_(self.classifier.bias)

-    def forward(self, input_ids, attention_mask, labels=None):
-        """Forward pass."""
-        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
+    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):
+        """Forward pass.

-        # Use [CLS] token representation
-        pooled_output = outputs.last_hidden_state[:, 0, :]
+        Args:
+            input_ids: Token IDs [batch_size, seq_len]
+            attention_mask: Attention mask [batch_size, seq_len]
+            token_type_ids: Segment IDs (optional, not used by RoBERTa/DeBERTa)
+            labels: Labels for loss computation (optional)
+
+        Returns:
+            Dict with 'logits' and optionally 'loss'
+        """
+        # Only pass token_type_ids if the model supports them
+        if self.uses_token_type_ids and token_type_ids is not None:
+            outputs = self.bert(
+                input_ids=input_ids,
+                attention_mask=attention_mask,
+                token_type_ids=token_type_ids
+            )
+        else:
+            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
+
+        # Use pooler_output if available, otherwise extract CLS token manually
+        # Detect pooler availability on first forward pass
+        if self.has_pooler is None:
+            self.has_pooler = hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None
+
+        pooled_output = outputs.pooler_output if self.has_pooler else outputs.last_hidden_state[:, 0, :]

         pooled_output = self.dropout(pooled_output)
         logits = self.classifier(pooled_output)
